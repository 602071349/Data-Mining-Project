group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()
View(bing_word)
write.csv(dat,file=myData.csv)
write.csv(dat,file="myData.csv")
write.csv(bing_word,file="myData1.csv")
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('app')
runApp('a')
runApp('app')
afinn <- tidy_tweets %>%
inner_join(get_sentiments("afinn")) %>%
group_by(id) %>%
summarise(sentiment = sum(score),number=n()) %>%
mutate(method = "AFINN")
set.seed(7)
dat<-afinn[,c(2,3)]
km1 = kmeans(dat, 4, nstart=100)
plot(dat, col =(km1$cluster +1) , main="K-Means result with 4 clusters", pch=20, cex=2)
View(dat)
runApp('app')
View(bing_word)
dat1<-bingword%<%
mutate(number=n)
dat1<-bingword%>%
mutate(number=n)
dat1<-bing_word%>%
mutate(number=n)
write.csv(dat,file="myData.csv")
write.csv(dat1,file="myData1.csv")
runApp('app')
View(dat1)
runApp('app')
View(dat1)
runApp('app')
View(dat1)
runApp('app')
View(dat1)
View(dat)
View(dat1)
View(dat1)
write.csv(dat,file="myData.csv")
write.csv(bing_word,file="myData1.csv")
View(dat1)
runApp('app')
View(dat)
runApp('app')
runApp('app')
write.csv(dat,file="myData.csv")
write.csv(bing_word,file="myData1.csv")
runApp('app')
afinn <- tidy_tweets %>%
inner_join(get_sentiments("afinn")) %>%
group_by(id) %>%
summarise(sentiment = sum(score),number=n()) %>%
mutate(method = "AFINN")
set.seed(7)
dat<-afinn[,c(2,3)]
km1 = kmeans(dat, 4, nstart=100)
plot(dat, col =(km1$cluster +1) , main="K-Means result with 4 clusters", pch=20, cex=2)
View(dat)
View(dat)
runApp('app')
View(dat)
runApp('app')
View(dat)
runApp('app')
afinn <- tidy_tweets %>%
inner_join(get_sentiments("afinn")) %>%
group_by(id) %>%
summarise(sentiment = sum(score),number=n()) %>%
mutate(method = "AFINN")
set.seed(7)
dat<-afinn[,c(2,3)]
km1 = kmeans(dat, 4, nstart=100)
plot(dat, col =(km1$cluster +1) , main="K-Means result with 4 clusters", pch=20, cex=2)
write.csv(dat,file="myData.csv")
write.csv(bing_word,file="myData1.csv")
runApp('app')
runApp('app')
View(dat)
write.csv(dat,file="myData.csv")
write.csv(bing_word,file="myData1.csv")
write.csv(dat,file="myData.csv")
write.csv(bing_word,file="myData1.csv")
a<-read.csv("myData.csv")
View(dat)
View(a)
write.csv(dat,file="myData.csv",row.names = FALSE)
write.csv(bing_word,file="myData1.csv")
runApp('app')
View(fre)
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
ggplot(fre, aes(x=word, y=proportion))+geom_point()
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()
View(tidy_tweets)
View(fre_words)
View(fre_words)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()
#calculate the propotion of top 15 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:15,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()
View(fre1)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()+scale_x_discrete(limits=fre1$word)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_line()+scale_x_discrete(limits=fre1$word)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_line()+gemo_point()+scale_x_discrete(limits=fre1$word)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_line()+geom_point()+scale_x_discrete(limits=fre1$word)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion,group = Section))+geom_line()+geom_point()+scale_x_discrete(limits=fre1$word)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()+geom_line()+scale_x_discrete(limits=fre1$word)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_line()+scale_x_discrete(limits=fre1$word)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_line()+scale_x_discrete(limits=fre1$word)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_pint()+scale_x_discrete(limits=fre1$word)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()+scale_x_discrete(limits=fre1$word)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()+scale_x_discrete(limits=fre1$word)+geom_abline()
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()+scale_x_discrete(limits=fre1$word)+geom_curve()
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()+scale_x_discrete(limits=fre1$word)+geom_path()
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()+scale_x_discrete(limits=fre1$word)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:50,]
ggplot(fre1, aes(x=word, y=proportion))+geom_line()+scale_x_discrete(limits=fre1$word)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_line()+scale_x_discrete(limits=fre1$word)
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_line()
#calculate the propotion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()+scale_x_discrete(limits=fre1$word)
#find words in tweets that represents joy sentiment.
nrcjoy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
joy<-tidy_tweets%>%
inner_join(nrcjoy) %>%
count(word, sort = TRUE)
kable(joy,"words of joy")
#find words in tweets that represents joy sentiment.
nrcjoy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
joy<-tidy_tweets%>%
inner_join(nrcjoy) %>%
count(word, sort = TRUE)
kable(joy,"words of joy")
#find words in tweets that represents joy sentiment.
nrcjoy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
joy<-tidy_tweets%>%
inner_join(nrcjoy) %>%
count(word, sort = TRUE)
kable(joy,caption = "words of joy")
nrcsadness<- get_sentiments("nrc") %>%
filter(sentiment == "sadness")
sadness<-tidy_tweets%>%
inner_join(nrcsadness) %>%
count(word, sort = TRUE)
kable(sadness,caption = "words of sadness")
tidy_tweets%>%
inner_join(get_sentiments("bing")) %>%
count(sentiment, sort = TRUE)
bing_word <- tidy_tweets %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()
tidy_tweets %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
tidy_tweets %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
kable(bing_word,caption = "popular words")
afinn <- tidy_tweets %>%
inner_join(get_sentiments("afinn")) %>%
group_by(id) %>%
summarise(sentiment = sum(score),number=n()) %>%
mutate(method = "AFINN")
set.seed(7)
dat<-afinn[,c(2,3)]
km1 = kmeans(dat, 4, nstart=100)
plot(dat, col =(km1$cluster +1) , main="K-Means result with 4 clusters", pch=20, cex=2)
View(a)
View(afinn)
write.csv(dat,file="app/myData.csv",row.names = FALSE)
write.csv(bing_word,file="app/myData1.csv")
write.csv(dat,file="app/myData.csv",row.names = FALSE)
write.csv(bing_word,file="app/myData1.csv")
write.csv(tweets.df,file="tweets.csv")
#Calculate the proportion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
ggplot(fre, aes(x = proportion, y = proportion)) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75")
theme(legend.position="none") +
fre1<-fre[1:10,]
#Calculate the proportion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
ggplot(fre, aes(x = proportion, y = proportion)) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75")
theme(legend.position="none")
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()+scale_x_discrete(limits=fre1$word)
View(fre)
#Calculate the proportion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
ggplot(fre, aes(x = n, y = proportion)) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75")
theme(legend.position="none")
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()+scale_x_discrete(limits=fre1$word)
#Calculate the proportion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()+scale_x_discrete(limits=fre1$word)
#Calculate the proportion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion))+geom_point()+geom_line()+scale_x_discrete(limits=fre1$word)
#Calculate the proportion of top 10 most frequent words among all words, and show it in a scatter plot.
fre <- tidy_tweets%>%
count(word, sort = TRUE) %>%
filter(n >50) %>%
filter(str_detect(word,"^[0-9]*$")==FALSE)%>%
mutate(word = str_extract(word, "[a-z']+")) %>%
mutate(proportion = n / sum(n))
fre1<-fre[1:10,]
ggplot(fre1, aes(x=word, y=proportion,group=1))+geom_point()+geom_line()+scale_x_discrete(limits=fre1$word)
#Tweets normally don't contain geographical information, so a map of data distribution isn't applicable in this case.
#Therefore, I use ggmap to draw a map of Alienware company's location, which is at Miami, Florida.
geocode("alienware")
#In this final project, I will explore a famous brand, Alienware, which sells gaming laptops and desktops. The main
#data resource are tweets about Alienware on twitter, and the main approach I adopt to explore the data is text #analysis,with tidytext package. The goal of this project is to figureout how people feel about this brand, and what
#words they use to describe this brand most frequently.
#Import necessary libraries.
library(devtools)
library(twitteR)
library(tidytext)
library(tidyverse)
library(stringr)
library(scales)
library(wordcloud)
library(reshape2)
library(knitr)
library(RgoogleMaps)
library(ggmap)
library(datasets)
#Set up twitter authentication.
api_key <- 	"T3QByVsO2YIhCHf5IWM7LbTeI"
api_secret <- "ozlLj87CJzJgypU4UGpxW2M4DyIImj1eaMyqZ8IZBk9fA6GW5N"
access_token <- "4237355133-MlNm9DcqK6ahkTwQ0gYDL6bCj8xn8plrGiqGp5d"
access_token_secret <- "lzXS9M6aJZAwEFOcU34VktJNexaE8DvGiGSlwhJgCbcNb"
setup_twitter_oauth(api_key,
api_secret,
access_token,
access_token_secret)
#Tweets normally don't contain geographical information, so a map of data distribution isn't applicable in this case.
#Therefore, I use ggmap to draw a map of Alienware company's location, which is at Miami, Florida.
geocode("alienware")
qmap(location = "alienware",zoom=14)
#Tweets normally don't contain geographical information, so a map of data distribution isn't applicable in this case.
#Therefore, I use ggmap to draw a map of Alienware company's location, which is at Miami, Florida.
geocode("alienware")
qmap(location = "alienware",zoom=14)
knitr::include_graphics('./alienware.png')
#Tweets normally don't contain geographical information, so a map of data distribution isn't applicable in this case.
#Therefore, I use ggmap to draw a map of Alienware company's location, which is at Hammocks,Florida.
geocode("alienware")
qmap(location = "alienware",zoom=14)
knitr::include_graphics('./alienware.jpg')
